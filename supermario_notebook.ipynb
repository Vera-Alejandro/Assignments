{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'nes_py'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-4-e08d4eb0c0ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnes_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJoypadSpace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym_super_mario_bros\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym_super_mario_bros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSIMPLE_MOVEMENT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nes_py'"]}],"source":["from nes_py.wrappers import JoypadSpace\n","import gym_super_mario_bros\n","from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n","import matplotlib.pyplot as plt\n","from skimage.transform import resize\n","import  numpy as np\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from collections import deque\n","from random import shuffle\n","import copy"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["env = gym_super_mario_bros.make('SuperMarioBros-v0')\n","env = JoypadSpace(env, SIMPLE_MOVEMENT)\n","\n","done = True\n","for step in range(0):\n","    if done:\n","        state = env.reset()\n","    state, reward, done, info = env.step(env.action_space.sample())\n","    env.render()\n","env.close()\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def downscale_obs(obs, new_size=(42,42), to_gray=True):\n","    if to_gray:\n","        return resize(obs, new_size, anti_aliasing=True).max(axis=2)\n","    else: \n","        return resize(obs, new_size, anti_aliasing=True)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def prepare_state(state):\n","    return torch.from_numpy(downscale_obs(state, to_gray=True)).float().unsqueeze(dim=0)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def prepare_multi_state(state1, state2):\n","    state1 = state1.clone()\n","    tmp = torch.from_numpy(downscale_obs(state2, to_gray=True)).float()\n","\n","    state1[0][0] = state1[0][1]\n","    state1[0][1] = state1[0][2]\n","    state1[0][2] = tmp\n","    return state1"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def prepare_initial_state(state, N=3):\n","    state_ = torch.from_numpy(downscale_obs(state, to_gray=True)).float()\n","    tmp = state_.repeat((N,1,1))\n","    return tmp.unsqueeze(dim=0)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def policy(qvalues, eps=None):\n","    if eps is not None:\n","        if torch.rand(1) < eps:\n","            return torch.randint(low=0, high=6, size=(1,))\n","        else: \n","            return torch.argmax(qvalues)\n","    else:\n","        return torch.multinomial(F.softmax(F.normalize(qvalues), dim=1), num_samples=1)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["class ExperienceReplay:\n","    def __init__(self, N=1000, batch_size=300):\n","        self.N = N\n","        self.batch_size = batch_size\n","        self.memory = []\n","        self.counter = 0\n","    \n","    def add_memory(self, state1, action, reward, state2, done):\n","        self.counter += 1\n","        \n","        if self.counter % 500 == 0:\n","            self.shuffle_memory()\n","\n","        if len(self.memory) < self.N:\n","            self.memory.append((state1, action, reward, state2, done))\n","        else: \n","            rand_index = np.random.randint(0, self.N-1)\n","            self.memory[rand_index] = (state1, action, reward, state2, done)\n","    \n","    def shuffle_memory(self):\n","        shuffle(self.memory)\n","\n","    def get_batch(self):\n","        if len(self.memory) < self.batch_size:\n","            batch_size = len(self.memory)\n","        else:\n","            batch_size = self.batch_size\n","\n","        if len(self.memory) < 1:\n","            print(\"Error: No data in memory.\")\n","            return None\n","\n","        ind = np.random.choice(np.arange(len(self.memory)), batch_size, replace=False)\n","        batch = [self.memory[i] for i in ind]\n","        state1_batch = torch.stack([x[0].squeeze(dim=0) for x in batch], dim=0)\n","        action_batch = torch.Tensor([x[1] for x in batch]).long()\n","        reward_batch = torch.Tensor([x[2] for x in batch])\n","        state2_batch = torch.stack([x[3].squeeze(dim=0) for x in batch], dim=0)\n","        done_batch = torch.Tensor([x[4] for x in batch])\n","\n","        return state1_batch, action_batch, reward_batch, state2_batch, done_batch"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class Qnetwork(nn.Module):\n","    def __init__(self):\n","        super(Qnetwork, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(32,32,kernel_size=(3,3), stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(32,32,kernel_size=(3,3), stride=2, padding=1)\n","        self.conv4 = nn.Conv2d(32,32,kernel_size=(3,3), stride=2, padding=1)\n","        self.linear1 = nn.Linear(288, 100)\n","        self.linear2 = nn.Linear(100, 7)\n","\n","    def forward(self, x):\n","        x = F.normalize(x)\n","        y = F.elu(self.conv1(x))\n","        y = F.elu(self.conv2(y))\n","        y = F.elu(self.conv3(y))\n","        y = F.elu(self.conv4(y))\n","        y = y.flatten(start_dim=2)\n","        y = y.view(y.shape[0], -1, 32)\n","        y = y.flatten(start_dim=1)\n","        y = F.elu(self.linear1(y))\n","        y = self.linear2(y)\n","        return y"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"output_type":"error","ename":"OSError","evalue":"exception: access violation reading 0x000000000003C208","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[1;32m<ipython-input-18-10c40307ad7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mstate1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_initial_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\RL_Class\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;34m\"\"\"Reset the environment and return the initial observation.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keys_to_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\RL_Class\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\.conda\\envs\\RL_Class\\lib\\site-packages\\nes_py\\nes_env.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;31m# reset the emulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_backup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\RL_Class\\lib\\site-packages\\nes_py\\nes_env.py\u001b[0m in \u001b[0;36m_restore\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;34m\"\"\"Restore the backup state into the NES emulator.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_will_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mOSError\u001b[0m: exception: access violation reading 0x000000000003C208"]}],"source":["# params = {\n","#     'batch_size': 450,\n","#     'gamma': 0.6,\n","#     'max_episode_len': 200,\n","#     'min_progress': 16,\n","#     'action_repeats': 6,\n","#     'frames_per_state': 3\n","# }\n","\n","# replay = ExperienceReplay(N=1500, batch_size=params['batch_size'])\n","# Qmodel = Qnetwork()\n","# model2 = copy.deepcopy(Qmodel)\n","# model2.load_state_dict(Qmodel.state_dict())\n","# sync_freq = 50\n","\n","# loss_fn = torch.nn.MSELoss()\n","# learning_rate = 1e-3\n","# optimizer = optim.Adam(Qmodel.parameters(), lr=learning_rate)\n","\n","# def reset_env():\n","#     env.reset()\n","#     state1 = prepare_initial_state(env.render('rgb_array'))\n","#     return state1\n","\n","# def minibatch_train():\n","#     state1_batch, action_batch, reward_batch, state2_batch, done_batch = replay.get_batch()\n","#     action_batch = action_batch.view(action_batch.shape[0],1)\n","#     reward_batch = action_batch.view(reward_batch.shape[0],1)\n","#     done_batch = action_batch.view(done_batch.shape[0],1)\n","    \n","#     qvals = Qmodel(state1_batch)\n","#     with torch.no_grad():\n","#         qtargets_ = model2(state2_batch)\n","\n","#     qtargets = reward_batch.squeeze() + params['gamma']*((1-done_batch.squeeze()) * torch.max(qtargets_, dim=1)[0])\n","#     X = qvals.gather(dim=1, index=action_batch).squeeze()\n","\n","#     return loss_fn(X, qtargets.detach())\n","\n","# eps = 1 \n","# losses = []\n","# ep_lengths = []\n","# e_reward = 0.0\n","# episode_length = 0\n","# epochs = 250000 \n","# env.reset()\n","# state1 = prepare_initial_state(env.render('rgb_array'))\n","# state_deque = deque(maxlen=params['frames_per_state'])\n","# last_x_pos = env.env.env._x_position\n","\n","\n","# for i in range(epochs):\n","#     optimizer.zero_grad()\n","#     episode_length += 1\n","#     qval_pred = Qmodel(state1)\n","#     action = int(policy(qval_pred, eps))\n","\n","#     for j in range(params['action_repeats']):\n","#         state2, e_reward_, done, info = env.step(action)\n","#         last_x_pos = info['x_pos']\n","#         if done:\n","#             state1 = reset_env()\n","#             break\n","#         e_reward += e_reward_\n","#         state_deque.append(prepare_state(state2))\n","    \n","#     state2 = torch.stack(list(state_deque), dim=1)\n","#     replay.add_memory(state1, action, e_reward, state2, done)\n","#     e_reward = 0\n","\n","#     if episode_length > params['max_episode_len']:\n","#         if (info['x_pos'] - last_x_pos) < params['min_progress']:\n","#             done = True\n","#         else:\n","#             last_x_pos = info['x_pos']\n","#     if done:\n","#         ep_lengths.append(info['x_pos'])\n","#         state1 = reset_env()\n","#         last_x_pos = env.env.env._x_position\n","#         episode_length = 0\n","#     else:\n","#         state1 = state2\n","    \n","#     if len(replay.memory) < params['batch_size']:\n","#         continue\n","\n","#     loss = minibatch_train()\n","#     losses.append(loss.item())\n","#     loss.backward()\n","#     optimizer.step()\n","\n","#     if i % 500 == 0:\n","#         print('epoch: ', i)\n","#         torch.save({\n","#             'Qmodel_dict': Qmodel.state_dict(),\n","#             'model2_dict': model2.state_dict(),\n","#             'optimizer_dict': optimizer.state_dict(),\n","#             'losses': losses,\n","#             'epoch': i\n","#             }, 'MarioModel.pt')\n","\n","#     if i % sync_freq == 0:\n","#         model2.load_state_dict(Qmodel.state_dict())\n","\n","#     if eps > 0.1:\n","#         eps -= (1/epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["checkpoint = torch.load('MarioModel.pt')\n","LoadedModel = Qnetwork()\n","LoadedModel.load_state_dict(checkpoint['Qmodel_dict'])\n","\n","losses_ = np.array(losses)\n","plt.figure(figsize=(8,6))\n","plt.xlim(0, len(losses_))\n","plt.ylim(0, max(losses_))\n","plt.plot(losses_, label='Q loss')\n","plt.legend()\n","plt.show()"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.11-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3.6.11 64-bit ('RL_Class': conda)","metadata":{"interpreter":{"hash":"d84fc8778ea45d101594afaab8973165e929a9f94cabf6e43493f57bf7b40f7a"}}}}}